# CLIP Fine-tuning with Adapters

<!--toc:start-->
- [CLIP Fine-tuning with Adapters](#clip-fine-tuning-with-adapters)
  - [ğŸ“ Project Structure](#ğŸ“-project-structure)
<!--toc:end-->

This repository contains code for fine-tuning CLIP  models using Adapters , allowing efficient parameter-efficient transfer learning. It also supports full fine-tuning by disabling adapters for comparison purposes. The architecture is modular and clean, making it easy to extend or customize for different datasets and tasks.

---

## ğŸ“ Project Structure

```
clip_adapter_finetune/
â”œâ”€â”€ Adapter/               # Contains adapter layers and utilities
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ clip_adapters.py   # Implementation of adapter modules for CLIP
â”œâ”€â”€ gen_label/             # Scripts to generate augmented labels using LLMs
â”‚   â””â”€â”€ label_generator.py
â”œâ”€â”€ model.py               # Main CLIP model with optional adapter support
â”œâ”€â”€ optim_factory.py       # Optimizer and scheduler factory
â”œâ”€â”€ main.py                # Entry point with argument parsing
â”œâ”€â”€ train.py               # Training loop (supports Apex and LoRA)
â”œâ”€â”€ eval.py                # Evaluation script on validation/test set
â””â”€â”€ README.md              # This file
```
